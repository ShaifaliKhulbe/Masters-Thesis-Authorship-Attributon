{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaifaliKhulbe/Masters-Thesis-Authorship-Attributon/blob/main/Hindi_baseline%2BPOS_(StandfordNLP).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGq02Hp7R9uf"
      },
      "outputs": [],
      "source": [
        "!pip install indicnlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07anEb_5SiZV"
      },
      "outputs": [],
      "source": [
        "!pip install indic-nlp-library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IciASeJeQBV7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import indicnlp\n",
        "import collections\n",
        "from nltk import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from itertools import chain\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from collections import Counter\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from indicnlp.tokenize import sentence_tokenize\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanfordnlp"
      ],
      "metadata": {
        "id": "qcPQwIkcZZJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import stanfordnlp\n",
        "stanfordnlp.download('hi')\n",
        "# initialize the Hindi pipeline for POS tagging\n",
        "nlp = stanfordnlp.Pipeline(processors='tokenize,pos', lang='hi')\n",
        "hindi_doc = nlp(\"\"\"केंद्र की मोदी सरकार ने शुक्रवार को अपना अंतरिम बजट पेश किया. कार्यवाहक वित्त मंत्री पीयूष गोयल ने अपने बजट में किसान, मजदूर, करदाता, महिला वर्ग समेत हर किसी के लिए बंपर ऐलान किए. हालांकि, बजट के बाद भी टैक्स को लेकर काफी कन्फ्यूजन बना रहा. केंद्र सरकार के इस अंतरिम बजट क्या खास रहा और किसको क्या मिला, आसान भाषा में यहां समझें\"\"\")"
      ],
      "metadata": {
        "id": "FVFpD13kZd30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qipc4rzAR9wv",
        "outputId": "e2e54c4e-5802-4682-9a3f-233672e85622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "291\n"
          ]
        }
      ],
      "source": [
        "with open('hindi_stopwords.txt', 'r', encoding='utf-8') as f:\n",
        "    stopwords = []\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:  # skip empty lines\n",
        "            stopwords.extend(line.split())\n",
        "print(len(stopwords))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VusvSf7GR9zY"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "df = pd.read_csv('200_chunks_Hindi.csv', encoding='utf-8')\n",
        "test_df = pd.read_csv('Test_200_chunks_Hindi.csv', encoding='utf-8')\n",
        "\n",
        "#only in the hindi csv, the very last row is empty hence this line below to be added\n",
        "df = df[:-1]\n",
        "\n",
        "def find_top_function_words(data, num):\n",
        "    \n",
        "    # tokenize each sentence and create a list of all words\n",
        "    words = []\n",
        "    for sentence in data['chunks']:\n",
        "        tokens = indic_tokenize.trivial_tokenize_indic(sentence)\n",
        "        words.extend(tokens)          \n",
        "\n",
        "    # count the frequency of each word in the entire CSV\n",
        "    word_freq = Counter(words)\n",
        "    \n",
        "    # sort the dictionary by frequency in descending order\n",
        "    sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # extract the 100 most frequent stopwords\n",
        "    top_stop_words = [word[0] for word in sorted_word_freq if word[0] in stopwords][:num]\n",
        "\n",
        "    return top_stop_words\n",
        "       \n",
        "#NEW    \n",
        "top_function_words = find_top_function_words(df, 100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.iloc[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whZQ-9MkddTL",
        "outputId": "8a0b78af-f466-4e5f-94ca-4152cbf227a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author_name                             सूर्यकांत त्रिपाठी निराला\n",
            "chunks            फिर चारों ओर से उमड़ी आती हुई सेना की आड़ मे...\n",
            "chunk_length                                                  239\n",
            "Name: 2671, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pos features of top pos ngrams per author"
      ],
      "metadata": {
        "id": "otuYiZJOvJCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define the dictionary to store all top ngrams for each author\n",
        "top_ngrams = {}\n",
        "\n",
        "#define the number of top pos ngrams to calculate\n",
        "num = 30 \n",
        "\n",
        "# calculate the pos ngrams for each author and store them in pos_ngrams_by_author\n",
        "for author in df['Author_name'].unique():\n",
        "    author_data = df[df['Author_name'] == author]\n",
        "    sentences = list(author_data['chunks'])\n",
        "\n",
        "    all_pos_1grams = []\n",
        "    all_pos_2grams = []\n",
        "    all_pos_3grams = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "\n",
        "        # Tokenize the sentence into lowercase words\n",
        "        tokens = indic_tokenize.trivial_tokenize_indic(sentence) #TOKENIZER\n",
        "        # tag the tokens with POS tags\n",
        "        #pos_tags = nltk.pos_tag(tokens, tagset='universal') #POS TAGGING\n",
        "\n",
        "        # convert the list of tokens to a string\n",
        "        text = ' '.join(tokens)\n",
        "        #print(text)\n",
        "        # analyze the sentence using StanfordNLP\n",
        "        doc = nlp(text)\n",
        "        # extract POS tags from the analyzed sentence\n",
        "        pos_tags = [(word.text, word.upos) for sent in doc.sentences for word in sent.words]\n",
        "        #print(pos_tags)\n",
        "\n",
        "        # create POS n-grams of size n\n",
        "        pos_unigram_list = ngrams([tag for _, tag in pos_tags], 1)\n",
        "        pos_bigram_list = ngrams([tag for _, tag in pos_tags], 2)\n",
        "        pos_trigram_list = ngrams([tag for _, tag in pos_tags], 3)\n",
        "\n",
        "        # add the POS n-grams to the list\n",
        "        all_pos_1grams.extend(pos_unigram_list)\n",
        "        all_pos_2grams.extend(pos_bigram_list)\n",
        "        all_pos_3grams.extend(pos_trigram_list)\n",
        "      \n",
        "  \n",
        "    # count the frequency of each POS n-gram\n",
        "    pos_1gram_counts = nltk.FreqDist(all_pos_1grams)\n",
        "    pos_2gram_counts = nltk.FreqDist(all_pos_2grams)\n",
        "    pos_3gram_counts = nltk.FreqDist(all_pos_3grams)\n",
        "    \n",
        "    # return the top num POS n-grams\n",
        "\n",
        "    top_unigrams= [ngram for ngram, count in pos_1gram_counts.most_common(num)]\n",
        "\n",
        "    top_bigrams= [ngram for ngram, count in pos_2gram_counts.most_common(num)]\n",
        "\n",
        "    top_trigrams= [ngram for ngram, count in pos_3gram_counts.most_common(num)]\n",
        "\n",
        "    top_ngrams[author] = top_trigrams + top_bigrams + top_unigrams\n",
        "\n",
        "#print(top_ngrams)"
      ],
      "metadata": {
        "id": "FudSSnGmu4xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the name of the first key in the dictionary\n",
        "first_key = list(top_ngrams.keys())[0]\n",
        "\n",
        "# Initialize the common_values set using the first key\n",
        "common_ngrams = set(top_ngrams[first_key])\n",
        "\n",
        "for key in top_ngrams:\n",
        "    common_ngrams = common_ngrams.intersection(top_ngrams[key])\n",
        "\n",
        "print(len(common_ngrams))\n",
        "\n",
        "# Create a list of all values from all keys minus the common values\n",
        "all_ngrams = []\n",
        "\n",
        "for key in top_ngrams:\n",
        "    all_ngrams.extend(top_ngrams[key])\n",
        "    \n",
        "print(len(all_ngrams))\n",
        "\n",
        "not_common_ngrams = set(all_ngrams).difference(common_ngrams)\n",
        "not_common_ngrams_list = list(not_common_ngrams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4__roTBhvOqM",
        "outputId": "af0d30f2-504d-40c7-c894-7c4ea648b1c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "53\n",
            "380\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VUelMbopvRbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbRM48tNR92k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7803582-1b95-4f41-ef96-332af3f72449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-f9cc16ecb149>:93: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['features'] = df['chunks'].apply(create_feature_vector)\n"
          ]
        }
      ],
      "source": [
        "def create_feature_vector(sentence):\n",
        "\n",
        "    \n",
        "    # Tokenize the sentence into lowercase words\n",
        "    tokens = indic_tokenize.trivial_tokenize_indic(sentence)\n",
        "\n",
        "    # Count the occurrences of each word in the sentence\n",
        "    word_counts = Counter(tokens)\n",
        "\n",
        "    # Create a feature vector based on the number of occurrences of the top 100 stopwords\n",
        "    top_function_vector = []\n",
        "    for stop_word in top_function_words:\n",
        "        count = word_counts.get(stop_word, 0)\n",
        "        top_function_vector.append(count)         \n",
        "              \n",
        "    # count the occurrences of each punctuation mark\n",
        "\n",
        "    punctuation_marks = [',',  '॥', '.', '!', '?', ':', ';', '।', '(', ')', '[', ']', '{', '}', 'ऽ', '—', '-', '‘', '’', '“', '”']\n",
        "\n",
        "    punctuation_counts = []\n",
        "    for mark in punctuation_marks:\n",
        "        count = sentence.count(mark)\n",
        "        punctuation_counts.append(count)\n",
        "\n",
        "    #pos features\n",
        "\n",
        "    # Tokenize the sentence into lowercase words\n",
        "    tokens = indic_tokenize.trivial_tokenize_indic(sentence) #TOKENIZER\n",
        " \n",
        "    # convert the list of tokens to a string\n",
        "    text = ' '.join(tokens)\n",
        "    #print(text)\n",
        "    # analyze the sentence using StanfordNLP\n",
        "    doc = nlp(text)\n",
        "    # extract POS tags from the analyzed sentence\n",
        "    pos_tags = [(word.text, word.upos) for sent in doc.sentences for word in sent.words]\n",
        "    \n",
        "    # create bigram, trigram, and four-gram POS sequences\n",
        "    unigram_pos_seqs = ngrams([tag for _, tag in pos_tags], 1)\n",
        "    bigram_pos_seqs = ngrams([tag for _, tag in pos_tags], 2)\n",
        "    trigram_pos_seqs = ngrams([tag for _, tag in pos_tags], 3)\n",
        "\n",
        "    # concatenate the three n-gram sequences into one\n",
        "    all_ngram_pos = chain(unigram_pos_seqs, bigram_pos_seqs, trigram_pos_seqs)\n",
        "\n",
        "    # create a list to store the counts of each n-gram in the not_common_ngrams_list\n",
        "    pos_counts = []\n",
        "\n",
        "    # concatenate the three n-gram sequences into one\n",
        "    all_ngram_pos = chain(unigram_pos_seqs, bigram_pos_seqs, trigram_pos_seqs)\n",
        "\n",
        "\n",
        "    # Converting all_ngram_pos to a list and printing the first 10 n-grams\n",
        "    all_ngrams_list = list(all_ngram_pos)\n",
        "\n",
        "\n",
        "    pos_counts = []\n",
        "    \n",
        "    for pos_ngram_list, top_pos_ngrams in zip([all_ngrams_list],\n",
        "                                              [not_common_ngrams_list]):\n",
        "        for pos_ngram in top_pos_ngrams:\n",
        "            count = 0\n",
        "            for top_pos in pos_ngram_list:\n",
        "                if pos_ngram == top_pos:\n",
        "                    count += 1\n",
        "            pos_counts.append(count)\n",
        "  \n",
        "    # create scaler objects for each feature type\n",
        "    punc_scaler = StandardScaler()\n",
        "    top_func_scaler = StandardScaler()\n",
        "    pos_scaler = StandardScaler()\n",
        "\n",
        "   # convert to NumPy arrays and reshape to have one column\n",
        "    punctuation_counts_reshaped = np.array(punctuation_counts).reshape(-1, 1)\n",
        "    top_function_vector_reshaped = np.array(top_function_vector).reshape(-1, 1)\n",
        "    pos_counts_reshaped = np.array(pos_counts).reshape(-1, 1)\n",
        "    \n",
        "    # fit and transform each feature type separately\n",
        "    punc_counts_scaled = punc_scaler.fit_transform(punctuation_counts_reshaped)\n",
        "    top_func_scaled = top_func_scaler.fit_transform(top_function_vector_reshaped)\n",
        "    pos_counts_scaled = pos_scaler.fit_transform(pos_counts_reshaped)\n",
        "    \n",
        "    # concatenate the scaled features\n",
        "    feature_vector = np.concatenate((punc_counts_scaled, top_func_scaled, pos_counts_scaled), axis=0)\n",
        "    \n",
        "    # convert back to a 1D array and return\n",
        "    return feature_vector.flatten()\n",
        "\n",
        "# create feature vectors for each sentence\n",
        "df['features'] = df['chunks'].apply(create_feature_vector)\n",
        "test_df['features'] = test_df['chunks'].apply(create_feature_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROmiY3AMR95r",
        "outputId": "4de798de-6d54-4310-a8e8-67faa7a9302a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM accuracy: 0.7563256325632564\n",
            "SVM F1-score: 0.7568789263400006\n",
            "SVM Precision: 0.772106026190086\n",
            "SVM Recall: 0.7563256325632564\n"
          ]
        }
      ],
      "source": [
        "X_train = np.array(df['features'].tolist())\n",
        "X_test = np.array(test_df['features'].tolist())\n",
        "\n",
        "y_train = df['Author_name']\n",
        "y_test = test_df['Author_name']\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_test_encoded = le.fit_transform(y_test)\n",
        "\n",
        "# create and train the SVM model\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, y_train_encoded)\n",
        "\n",
        "# Make predictions on the test set and calculate evaluation metrics\n",
        "y_pred = svm.predict(X_test)\n",
        "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
        "f1 = f1_score(y_test_encoded, y_pred, average='weighted')\n",
        "precision = precision_score(y_test_encoded, y_pred, average='weighted')\n",
        "recall = recall_score(y_test_encoded, y_pred, average='weighted')\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"SVM accuracy:\", accuracy)\n",
        "print(\"SVM F1-score:\", f1)\n",
        "print(\"SVM Precision:\", precision)\n",
        "print(\"SVM Recall:\", recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5DtP4sfR98o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f787f16a-e314-42cb-96bf-98339c2c9863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.92      0.85       526\n",
            "           1       0.84      0.70      0.76       682\n",
            "           2       0.68      0.74      0.71       235\n",
            "           3       0.86      0.63      0.72       185\n",
            "           4       0.51      0.66      0.58       190\n",
            "\n",
            "    accuracy                           0.76      1818\n",
            "   macro avg       0.74      0.73      0.72      1818\n",
            "weighted avg       0.77      0.76      0.76      1818\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test_encoded, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# num = 50 is the best\n"
      ],
      "metadata": {
        "id": "JAsSw0kfxE5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num = 75\n",
        "\n",
        "SVM accuracy: 0.7596259625962596\n",
        "SVM F1-score: 0.759707950479026\n",
        "SVM Precision: 0.7754689834442013\n",
        "SVM Recall: 0.7596259625962596\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.78      0.90      0.83       526\n",
        "           1       0.82      0.68      0.74       682\n",
        "           2       0.74      0.77      0.75       235\n",
        "           3       0.91      0.63      0.74       185\n",
        "           4       0.55      0.76      0.64       190\n",
        "\n",
        "    accuracy                           0.76      1818\n",
        "   macro avg       0.76      0.75      0.74      1818\n",
        "weighted avg       0.78      0.76      0.76      1818\n",
        "\n",
        "num = 50\n",
        "\n",
        "SVM accuracy: 0.7722772277227723\n",
        "SVM F1-score: 0.7762332805662419\n",
        "SVM Precision: 0.7936031440900685\n",
        "SVM Recall: 0.7722772277227723\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.87      0.91      0.89       526\n",
        "           1       0.84      0.72      0.78       682\n",
        "           2       0.76      0.83      0.79       235\n",
        "           3       0.79      0.58      0.67       185\n",
        "           4       0.46      0.71      0.56       190\n",
        "\n",
        "    accuracy                           0.77      1818\n",
        "   macro avg       0.74      0.75      0.74      1818\n",
        "weighted avg       0.79      0.77      0.78      1818\n",
        "\n",
        "\n",
        "num = 30\n",
        "\n",
        "SVM accuracy: 0.7563256325632564\n",
        "SVM F1-score: 0.7568789263400006\n",
        "SVM Precision: 0.772106026190086\n",
        "SVM Recall: 0.7563256325632564\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.78      0.92      0.85       526\n",
        "           1       0.84      0.70      0.76       682\n",
        "           2       0.68      0.74      0.71       235\n",
        "           3       0.86      0.63      0.72       185\n",
        "           4       0.51      0.66      0.58       190\n",
        "\n",
        "    accuracy                           0.76      1818\n",
        "   macro avg       0.74      0.73      0.72      1818\n",
        "weighted avg       0.77      0.76      0.76      1818\n",
        "\n",
        "\n",
        "num = 100"
      ],
      "metadata": {
        "id": "9JIsBc6nhqpA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxm3hKfsVmITc9BaKqgBmm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}